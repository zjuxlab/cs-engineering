---
title: æœç´¢ç®—æ³•
slug: >-
  l3nnwj3ftiporjkfsqacaj4znl4-thmlw28zjigez1kgotycuenonrc-y6gvwxdaniwuhukbkh5cqyk9nud-fupnwsj9bi8ziektuaiczzh2nde-fupnws
sidebar_position: 0
---


# æœç´¢ç®—æ³•

æœç´¢ï¼Œå³é€šè¿‡ä¸€å®šç­–ç•¥çš„éå†å„ç§æƒ…å†µï¼Œæ¥å¾—åˆ°ä¸€åœºæ¸¸æˆçš„æœ€ä¼˜å†³ç­–æ–¹å¼

# åŸºæœ¬æ¦‚å¿µ

## å†³ç­–æ¨¡å‹

- çŠ¶æ€ç©ºé—´State Spaceï¼šå½“å‰æ‰€å¤„çš„ä½ç½®
- è¡ŒåŠ¨ç©ºé—´Action Spaceï¼šå½“å‰å¯ä»¥é‡‡å–çš„å†³ç­–è¡ŒåŠ¨
- å¥–åŠ±å‡½æ•°Reward Funcï¼šæ‰§è¡Œå½“å‰æ­¥éª¤çš„ä»·å€¼
- å†³ç­–æ¨¡å‹ï¼š
    - è¿›è¡Œå†³ç­–ï¼šAction = Decide(State, Reward)
    - å®è¡Œå¹¶è§‚æµ‹ï¼šState , Reward  = Step(State, Action)

<div class="callout callout-bg-2 callout-border-2">
<div class='callout-emoji'>ğŸ’¬</div>
<p>Eg. ä¸‹æ£‹</p>
<p>çŠ¶æ€ç©ºé—´ï¼šæ£‹ç›˜ä¸Šè½å­æƒ…å†µ</p>
<p>è¡ŒåŠ¨ç©ºé—´ï¼šåœ¨ä¸åŒå¯è½å­çš„åœ°æ–¹è½å­</p>
<p>å¥–åŠ±ï¼šå½“å‰æ—¶åˆ»ç»“æŸçš„è¯ï¼Œæ€»å¾—åˆ†ï¼ˆæ„Ÿè§‰å¯ä»¥æœ‰å¾ˆå¤šç§å¥–åŠ±å‡½æ•°ï¼‰</p>
</div>

## æœç´¢æ¨¡å‹

çŠ¶æ€æ˜¯èŠ‚ç‚¹ï¼Œè¡Œä¸ºæ˜¯æœ‰å‘è¾¹

- æ ‘æ¨¡å‹ï¼šæ¯ä¸€å±‚æ˜¯ä¸€ä¸ªæ—¶åˆ»çš„çŠ¶æ€ï¼Œå­èŠ‚ç‚¹è¡¨ç¤ºå†³ç­–åçš„æ–°çŠ¶æ€
    - å­˜åœ¨çŠ¶æ€èŠ‚ç‚¹é‡å¤ï¼Œä¸æ˜“å¤ç”¨ï¼›ä¾¿äºè¡¨ç¤ºéšå†³ç­–è¿›åº¦è¡ŒåŠ¨ç©ºé—´ä¼šå‘ç”Ÿå˜åŒ–çš„æƒ…å†µ

- å›¾æ¨¡å‹ï¼šè¡¨ç¤ºçŠ¶æ€ä¹‹é—´æ‰€æœ‰å¯èƒ½çš„è½¬ç§»å…³ç³»
    - çŠ¶æ€èŠ‚ç‚¹å’Œè¡Œä¸ºå”¯ä¸€ï¼Œæ–¹ä¾¿å­˜å‚¨å’Œä¼˜åŒ–ï¼›ä¾¿äºè¡¨ç¤ºå¤šæ¬¡é‡å¤åšå¼ˆ

# å…¸å‹æœç´¢ç®—æ³•

## BFS

å°±ä¸è¯´äº†ï¼Œä¼˜åŒ–ä¹‹åæ˜¯A*

## DFS

å°±ä¸è¯´äº†ï¼Œä¼˜åŒ–ä¹‹åæ˜¯Iterative Deepening Search

## A-Star

å¯å‘å¼ä¼˜åŒ–çš„BFSæœç´¢

### æ•´ä½“æ€è·¯

è®¾è®¡ä¸€ä¸ªå¯å‘å‡½æ•°H(S)

ç»´æŠ¤ä¸€ä¸ªä¼˜å…ˆé˜Ÿåˆ—PQ

æŒ‰ç…§å¦‚ä¸‹é¡ºåºéå†èŠ‚ç‚¹

```bash
PQ.add(0, S0)
while not PQ.empty():
    S = PQ.popmax()
    do_something_with(S)
    for N in S.neighbors:
        h = H(x) + Cost(S)
        PQ.add(h, N)
```

### å¯å‘å‡½æ•°çš„è®¾è®¡

æ‰€è°“çš„å¯å‘å¼ï¼Œå®é™…ä¸Šæ˜¯åˆ©ç”¨çš„é—®é¢˜ç‹¬ç‰¹çš„ç‰¹æ€§ï¼Œæ¥æŒ‡å¼•æœç´¢çš„å¤§ä½“æ–¹å‘

å¯å‘å‡½æ•°å…·æœ‰å¦‚ä¸‹<b>å¿…è¦æ€§è´¨ï¼š</b>

- Admissiveï¼šã€ä¹è§‚ä¼°è®¡ã€‘è¦æ±‚H(x)å°äºxåˆ°ç»ˆç‚¹çš„æœ€çŸ­å¼€é”€
- Consistentï¼šH(x)ä¸xåˆ°ç»ˆç‚¹çš„æœ€çŸ­å¼€é”€ä¸€åŒå‡å°
- Preciseï¼šã€ä¹è§‚ä¼°è®¡ã€‘è¦å°½é‡è´´è¿‘çœŸå®æœ€çŸ­è·¯å¾„æƒ…å†µ
    - å¦‚æœH(x)=0, åˆ™é€€åŒ–ä¸ºBFS
    - å¦‚æœH(x)=æœ€çŸ­è·¯å¾„ï¼Œåˆ™ä¸å¯åœ¨å¸¸æ•°æ—¶é—´å®ç°

åœ¨æ¬§æ°è·ç¦»ä½œä¸ºå¯å‘å‡½æ•°æ—¶ï¼Œç»™å‡ºä¸‰æ¡æ€§è´¨çš„å…·ä½“è¡¨è¿°

- ç›´çº¿è·ç¦»æ¯”æŠ˜çº¿è·ç¦»çŸ­
- ç›´çº¿è·ç¦»å’Œæœ€çŸ­æ›²çº¿è·ç¦»åŒæ—¶å‡å°
- é è¿‘ç»ˆç‚¹æ—¶ï¼Œå¯å‘å‡½æ•°è¿‘ä¼¼ç­‰äºçœŸå®è·ç¦»

## Iterative Deepening Search

åŸºäºDFSæ ‘æœç´¢ä¼˜åŒ–ï¼Œæ¯ä¸€è½®é€’å¢æ·±åº¦dï¼Œåªæœç´¢æ·±åº¦dçš„èŠ‚ç‚¹

## Uniform Cost Search    [UCS]

æ˜¯dijkstraçš„å¢å¹¿æ¨¡å‹ï¼›æ¯ä¸ªç‚¹çš„å†å²æ€»æƒé‡åŸºäºæœç´¢è·å¾—ï¼Œä¸ä¸€å®šæ˜¯åˆ°èµ·ç‚¹çš„æœ€çŸ­è·ç¦»

- æ¯æ¬¡ç»´æŠ¤frontieræ•°ç»„ï¼Œè¡¨ç¤ºå·²æ›´æ–°è¿‡valueçš„èŠ‚ç‚¹
- æ¯ä¸€è½®ä»frontierä¸­éå†èŠ‚ç‚¹è¿›è¡Œä¸‹ä¸€æ­¥æœç´¢

## MonteCarlo Tree Search    [MCTS]

### è’™ç‰¹å¡æ´›æ–¹æ³•

- åŸºäºå¤§æ•°å®šå¾‹ï¼Œä½¿ç”¨éšæœºåŒ–çš„é‡‡æ ·ç»“æœè¿‘ä¼¼ä»£æ›¿çœŸå®ç»“æœçš„ä¸€ç±»ç®—æ³•è®¾è®¡æ€æƒ³

## ç®—æ³•æµç¨‹

- Selection é€‰æ‹©<b>æœ€æƒ³æ¢ç´¢</b>çš„å­èŠ‚ç‚¹
    - argmaxï¼šå„ä¸ªå¶èŠ‚ç‚¹çš„<b>è¯„åˆ†rank</b>

- Expansion ç”Ÿæˆä¸€ä¸ª<b>ä¸‹ä¸€æ­¥å†³ç­–</b>
- Simulation 
    - åˆ°æ­¤èŠ‚ç‚¹çš„è·¯å¾„ä¸ºå‰Næ¬¡<b>å·²çŸ¥</b>å†³ç­–
    - ä»æ­¤å†³ç­–å¼€å§‹ï¼Œ<b>éšæœº</b>å‘ä¸‹<b>æ¨¡æ‹Ÿå†³ç­–</b>
    - å¾—åˆ°è¯•éªŒ<b>å¾—åˆ†</b>

- BackPropagation
    - åŸºäºè¯•éªŒ<b>å¾—åˆ†</b>ï¼Œåˆ·æ–°æœç´¢æ•°å„çº§çˆ¶äº²çš„<b>è¯„åˆ†rank</b>

- å¾ªç¯1-4æ­¥ï¼Œç›´åˆ°è¾¾åˆ°ç»“æŸæ¡ä»¶
    - ç»“æŸæ¡ä»¶ï¼šå¾ªç¯æ¬¡æ•°ã€ç»“æœç²¾åº¦ç­‰

## èŠ‚ç‚¹è¯„åˆ†è®¡ç®—

### <b>æ¢ç´¢å’Œåˆ©ç”¨</b>

- åœ¨é€‰æ‹©èŠ‚ç‚¹æ—¶ï¼Œé€šå¸¸é¢å¯¹çš„ä¸€ä¸ªé—®é¢˜æ˜¯<b>æ¢ç´¢å’Œåˆ©ç”¨(Exploration & Exploitation)</b>çš„å†²çª
    - å½“åå‘æ¢ç´¢ï¼Œåˆ™æœç´¢æ”¶æ•›é€Ÿåº¦éå¸¸æ…¢
    - å½“åå‘åˆ©ç”¨ï¼Œåˆ™å¯èƒ½å› ä¸ºè´ªå¿ƒç­–ç•¥é™·å…¥å±€éƒ¨æœ€ä¼˜è§£

- ä¸ºäº†è§£å†³æ­¤å†²çªï¼Œæˆ‘ä»¬éœ€è¦ä¸€äº›åˆç†çš„è¯„åˆ†ç®—æ³•ï¼Œè¯¥ç®—æ³•
    - åœ¨æœç´¢æ—©æœŸæ³¨é‡éšæœºåŒ–æ¢ç´¢
    - åœ¨æœç´¢æœ«æœŸæ³¨é‡æœ€ä¼˜è§£é‚»åŸŸè¿›è¡Œä¼˜åŒ–

### æ¨¡æ‹Ÿé€€ç«

ä¸€ç§æ˜¾ç„¶çš„æƒ³æ³•ç±»ä¼¼äºæ¨¡æ‹Ÿé€€ç«

- æ ¹æ®æœç´¢è¿›åº¦ï¼Œåœ¨è´ªå¿ƒç­–ç•¥ä¸­ä»¥é€’å‡çš„æ¦‚ç‡pæ¥å—éæœ€ä¼˜ç»“æœ
- ç›´åˆ°æœ€åpè¶‹è¿‘äº0ï¼Œç»“æœæ”¶æ•›åˆ°æœ€ä¼˜æœç´¢ç‚¹

### UCB

å¦å¤–ä¸€ç§å¸¸ç”¨çš„ç®—æ³•æ˜¯<b>Upper Confidence Bound   [UCB]</b>

å…¶æ•°å­¦æ¨å¯¼è¾ƒä¸ºå¤æ‚ï¼Œåœ¨æ­¤ç®€è¿°å…¶<b>æ ¸å¿ƒæ€æƒ³</b>

- ç½®ä¿¡åº¦Cæ•°æ®é›†å¤§å°çš„å½±å“
    - `C = sqrt(2*ln(cnt_visit[parent])/cnt_visit[curr])`

- ä»¤è¯„åˆ†ä¸ºå†å²æœç´¢å¹³å‡å€¼åœ¨ç½®ä¿¡åº¦ä¸‹çš„ä¸Šç•Œï¼Œå³rank = avg(value) + Cã€é‚£ä¸ªæ ¹å·ä¸‹logçš„å¤æ‚ä¸œè¥¿ã€‘
    <img src="/assets/Wn5pbkJ6mo4VZrx315qcyjzKn0S.png" src-width="1719" src-height="663" align="center"/>

- åˆ™rankçš„å¤§å°å³åŒæ—¶è¡¨å¾äº†æ¢ç´¢æ¬¡æ•°å’Œæ¢ç´¢å¾—åˆ†ï¼Œä¸”éšæ—¶é—´æ¨ç§»è¶‹å‘äºæ¢ç´¢å¹³å‡åˆ†æ•°å­¦æœŸæœ›

- UCTï¼šå› ä¸ºUCBå’ŒMCTSç»“åˆçš„ç®—æ³•å¤ªè¿‡å¥½ç”¨å’Œå¸¸ç”¨ï¼Œä»–ç”šè‡³æœ‰äº†å•ç‹¬çš„åå­—â€”â€”<b>UCT</b>

### 
ä»£ç æ ·ä¾‹

```py
# algo.py
import math
from random import randint, shuffle
from typing import Callable, Iterable

from gym.core import ActType
from treelib import Node, Tree

ValueFunction = Callable[[Iterable[ActType]], float]


class NodeData:
    def __init__(self, act: ActType) -> None:
        self.act = act
        self.is_end: bool = False
        self.cnt_visit: int = 0
        self.cnt_simulate: int = 0
        self.total_reward: float = 0
        self.quality: float = 0
        self.is_end: bool = False
        self.tried_acts: list[ActType] = []


class MCTS:
    def __init__(self, simulate_callback: ValueFunction, actions: list[ActType]) -> None:
        self.tree = Tree()
        self.tree.create_node("ROOT", "ROOT", data=NodeData(None))
        self.simulation_func = simulate_callback
        self.round: int = 0
        self.actions = actions
        self.max_reward = 0

    def run(self, total_rounds: int) -> list[ActType]:
        root_node = self.tree[self.tree.root]
        for self.round in range(total_rounds):
            if self.round % 100 == 0:
                print(f"round {self.round}")
            old_leaf = self.select(root_node)
            if old_leaf is None:
                print("old_leaf is None!")
                continue
            new_leaf = self.expand(old_leaf)
            if new_leaf is None:
                print("new_leaf is None!")
                continue
            reward = self.simulate(new_leaf)
            # print(f"score = {reward}")
            if self.max_reward < reward:
                print(f"max reward: {reward}")
                self.max_reward = reward
            self.back_propagate(new_leaf, reward)

        best_leaf = self.select(root_node)
        return self.acts_to_node(best_leaf)

    def select(self, root: Node) -> Node:
        while True:
            childs = self.tree.children(root.identifier)
            if len(childs)<2:
                return root

            best_score = -100
            best_node: Node = None
            for child in childs:
                score = self.node_score(child)
                if score >= best_score :
                    best_score, best_node = score, child
                    
            if best_node is None:
                root.data.is_end = True

            root = best_node

    def expand(self, node: Node) -> Node:
        if node is None or len(node.data.tried_acts) == len(self.actions):
            return node

        # éšæœºç­›é€‰ä¸€ä¸ªæ²¡å°è¯•è¿‡çš„act
        acts = list(
            filter(lambda act: act not in node.data.tried_acts, self.actions))
        shuffle(acts)
        act = acts[0]
        node.data.tried_acts.append(act)
        new_node = self.tree.create_node(data=NodeData(act), parent=node)
        return new_node

    def simulate(self, node: Node) -> float:
        if node.data.is_end:
            return 0

        known_acts = self.acts_to_node(node)

        # s = "".join(["l" if act==0 else "r" for act in known_acts])
        # print(f"acts: {s}")
        # print(f"len(acts)={len(known_acts)}")

        def iter_acts():
            for act in known_acts:
                yield act
            while True:
                yield randint(0, 1)
        score = self.simulation_func(iter_acts())
        return score

    def back_propagate(self, node: Node, reward: float):
        path = self.path_to_node(node)
        if reward < len(path)*0.8:
            node.data.is_end = True
            return

        node.data.cnt_simulate += 1

        for node in path:
            node.data.total_reward += reward
            node.data.cnt_visit += 1


    def path_to_node(self, node: Node) -> list[Node]:
        if node is None: return []
        nodes = [node]
        p = node
        while p := self.tree.parent(p.identifier):
            if p.is_root():
                break
            nodes.append(p)
        nodes.reverse()
        return nodes

    def acts_to_node(self, node: Node) -> list[ActType]:
        return [n.data.act for n in self.path_to_node(node)]

    def node_score(self, node: Node) -> float:
        pnode = self.tree.parent(node.identifier)

        data: NodeData = node.data
        pdata: NodeData = pnode.data if pnode is not None else data
        k = 1.4
        return (data.total_reward/(data.cnt_visit+1)) + k * math.sqrt(math.log2(pdata.cnt_visit+2)/(data.cnt_visit+1))
```

```py
# main.py
from time import sleep
from typing import Iterable

import gym
from gym.core import ActType

from algo import MCTS

env = gym.make("CartPole-v1")

def value_func(actions: Iterable[ActType], delay=.0) -> float:
    total_reward = 0
    env.reset(seed=1)
    for action in actions:
        observation, reward, terminated, truncated, info = env.step(action)
        total_reward += reward
        # print(f"action={action}, observation={observation}, reward={reward}")
        if terminated or truncated:
            observation, info = env.reset()
            break
        sleep(delay)
    if total_reward<20: return 0
    return total_reward

mcts = MCTS(value_func, [0,1])
best = mcts.run(1000)

env = gym.make("CartPole-v1", render_mode="human")
env.reset(seed=1)
while True:
    value_func(best, 0.025)
    sleep(1)
```

