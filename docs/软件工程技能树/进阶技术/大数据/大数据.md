---
title: 大数据
slug: 大数据
sidebar_position: 5
---


# 大数据

Author：赵紫宸

看到又是“大数据”，又是“计算引擎”，你可能顿时感觉这是个高级玩意儿，离自己相当遥远。我还记得在我大二的时候，一听到大数据，就有一种云里雾里、琢磨不透的感觉。

<div class="callout callout-bg-2 callout-border-2">
<div class='callout-emoji'>💡</div>
<p>一项技术之所以流行，必然是因为它较好地解决了工业界稳定存在的刚性需求。</p>
<p>大数据离我们并不遥远。</p>
</div>

<div class="callout callout-bg-2 callout-border-2">
<div class='callout-emoji'></div>
<p>随着我们拿到手里的服务日益复杂化，后端和运维的同学可能遇到一些需要分布式部署的场景。出于这一点考虑，我觉得适当拓宽大家的领域知识还是比较重要的。毕竟，分布式系统、一致性、容灾这些概念，在很多实践中都是相通的。如果你完全没有接触这个领域，也没有关系，本次分享主要是希望大家脑子里有这些概念，未来需要进行相关的思考时，才能更“有章可循”。</p>
</div>

## 从“小数据”谈起

想象你有如下 JSON 数组，每一个数组元素，都代表了一个单词和它出现的次数。你的目标是求出每个单词出现的总次数。

```json
[
    {
        "word": "alpha",
        "count": 1
    },
    {
        "word": "beta",
        "count": 1
    },
    {
        "word": "alpha",
        "count": 1
    }
]
```

一般情况下，我们用 JS 或者 Python 写一个小脚本，就可以对它进行处理，还是比较高效的。哪怕有一万条记录，也完全不慌，读了文件以后，用脚本语言处理一下也是可以的。

然而，在实际生产中，我们遇到的问题是：这样的记录，以每秒百万条的量级被发送过来。在这种情况下，出于业务需要，你还是需要拿到单词出现的次数才方便做后面的分析。那么在这种情况下，别说脚本语言了，你在一台服务器上部署实现这一过程的 C++ 代码，都是杯水车薪。

我们观测到了如下几个问题：

1. 在我们原本熟悉的领域，无论是处理 JSON 文件，还是加载一些训练集，我们常见的数据往往都是写死在文件里的。这种数据被称为有界数据。而在现实生活中，很多数据往往是无界 (boundless) 的，是源源不断地流过来的。
2. 这种数据的量非常大，单个节点无论如何也承受不了如此庞大的计算量。
3. 一些计算需求，比如 map filter reduce，是稳定存在的。

基于此，我们有必要提出一种新的系统。它能：

1. 能够应对无界数据
2. 是一个分布式系统
3. 支持常用的计算方式，或者更专业地讲，常用的算子 (operator)

这样，我们采集到的原始数据，就在这个专门负责计算的系统中进行了 pre-process，然后再把数据交付给下游的系统，进行其他处理。

## 大数据语境下的 WordCount 实现

### 简要概览

以下图片展示了在分布式 WordCount 的实现。这里的输入其实更为原始，是一串字符流`XBBCBAXAC`。面对这样的输入，首先我们截取一定时间内的字符流，并对它进行随机拆分 (split) 操作。这样，数据就被分散到三个不同的节点上了。随后，我们把 `(word)` 映射 (map) 为二元组 `(word, count)` ，这样就方便后面的求和计算了。具体怎么求和呢？我们看到，可以对这一步的结果进行哈希分区 (hash partition) ，得到的效果是，所有节点上的A都被聚集在了同一个节点上，这样才能有求和的可能。最后，对于每个节点上的每个 word 数进行求和，就得到了最终结果。

![](/assets/SNTJbvJqqon0etxqbIecVMJmnxf.png)

先让我们思考一下，大数据计算引擎到底扮演了什么角色。事实上，刚刚提到的这些逻辑都是数据分析师、开发者自己写出来的。比如，以伪代码形式呈现的话：

```java
SourceStream
    .split(3)
    .map(x -> Tuple(x, 1))
    .hashBy(x -> x.word)
    .sum(x -> x.count)
```

split、map 这些，就是我们上文中提到的算子。这段代码逻辑，会被计算引擎解析成上面的 DAG 图。随后，计算引擎会向所在环境（往往是一个拥有多个节点的集群）申请资源，并将这个 DAG 图进行部署（这意味着，在机器上真的存在三个线程，用来管理 split 后的结果）。同时，连接数据源（对于我们这个例子，就是字符流）和数据漏。

### 数据分区 partitioning

分区方式决定了上游数据以何种策略被分配到下游。上面提到的作业执行流程中，存在两次节点“少到多”的过程：

1. 把数据源进行 split
2. 把 map 好的数据进行 combine

容易观察到，它们各有特点：

1. split 的时候，我们完全不需要考虑数据的有序性。这是因为，我们在这步的目的仅仅是把数据分散到不同节点上，所以在做这步时，可能我们只需要循环地 (round-robin) 把数据分给下游节点即可。
2. 而 combine 的时候，数据的重分配就有讲究了。因为我们下一步要求和，所以我们显然希望相同的 word 出现在相同节点上，否则不可能完成求和。这时，我们就选择了用哈希 (hash) ，只需保证每个上游节点的哈希函数相同，那么数据自然地就会在下游节点完成哈希分布。

不同的大数据引擎对于这两种分区方式有不同的称呼。比如，Flink 称前者为 Rebalance ，后者为 KeyGroup/Hash ，如果你在未来在企业接触到人工智能和大数据并碰到了 Flink，这些都是再常见不过的概念了，好比 Softmax 之于机器学习的地位。

## 运行时 runtime

上面的描述其实还是比较抽象的，具体而言，我并没有解释作业 DAG 图到底是怎么部署到集群中的。

这其实不是一个理论问题，而是一个工程实践问题了。以我比较熟悉的 Flink 为例，我们把作业 DAG 图旋转 90 度，得到下面这样的一张图。随后，让我们想象，节点被用以下方式染上了颜色。相同颜色的节点被放到了相同的槽 (slot) 内，一个 TaskManager 可能有多个槽。而 TaskManager，是一个 Kubernetes 的 pod （编排调度引擎 Kubernetes 在此不做展开）；如果不理解 pod，把它想象成一个容器即可。

![](/assets/UbIrby92worxWixIIPYc5EQrn1U.png)

## SQL

让我们再回顾一下我们刚刚遇到的问题。我们要统计单词出现的次数，数据以 `(word, count)` 的形式出现。如果这些数据是存在 MySQL 里的，那么显然，只需要一条 SQL 语句就可以完成这项任务：

```sql
SELECT word, SUM(count) AS sum FROM source GROUP BY word;
```

而且，现实情况是，大数据计算引擎的用户，在很多时候是数据分析师。他们很多是从数学等专业毕业的，不具备非常强的开发能力。因此，刚刚我们提到写代码去生成作业 DAG 图，对他们来说可能比较困难。那么，自然而然地，较为现代的计算引擎就可以接受 SQL ，把 SQL 语句解析为我们刚刚的那个 DAG 图。这也是 Flink 在生产中最常见的使用方式。

<div class="callout callout-bg-2 callout-border-2">
<div class='callout-emoji'>💡</div>
<p>如果你去问做大数据应用的人，他们可能会说，Flink 啊，写写 SQL 就完事了。即使如此，我们依然有必要了解底层的概念，这样我们才能真正明白计算引擎是如何工作的。</p>
</div>

## 容灾 fault tolerance

擅长独立思考的同学，可能已经意识到我没有提及，但依然很重要的一些点：

1. 既然 TaskManager 是在容器里运行的，那么万一有哪天，突然这容器就挂了，那整个计算流程岂不是就断了？
2. 仔细想想，既然数据是流过不同节点的，但容器一挂，节点没了，假设我要恢复计算流程，我怎么知道我先前已经处理到哪儿了？

与容灾、状态、一致性相关的问题，是计算引擎的一个重点。其中涉及到的知识点较多，在这里就不做展开了。目前你只需要知道，计算引擎通过一套较为完善的 state & checkpoint 体系，很大程度上保证了这些性质，也欢迎大家在这些方面展开自己的探索。

下面贴几张与之相关的截图。

![](/assets/YNLMbYZr9oWpQZx4alxcvSEongb.png)

![](/assets/ZY6zbSKXUoG66mxevwQcGjLlnOh.png)

## 结语

如果你修读过数据库课程（浙大大二下），你会有种感觉：计算引擎就好比是一个不考虑存储功能的、分布式的数据库。事实上，计算引擎的数据源可以是数据库，那么数据就是有界的；也可以是一个源源不断生产数据的数据流，也许你听说过 Apache Kafka 和 RoketMQ ，它们都是这样的工具，我们称它们为消息队列。凭什么它们能源源不断地生产数据？这还是因为，有人在不断地向这些打数据。

想象这样一个业务链路：

1. 抖音app客户端持续向消息队列发送用户观看时长、用户点赞、用户转发等原始信息
2. 机器学习团队为了训练推荐模型，于是就写了一个计算引擎任务，它不断地从消息队列里拉取数据，进行预处理，然后将结果输出到机器学习团队的文件系统中。
3. 有一个做数据可视化的团队，希望能写 SQL 分析用户行为数据并进行展示，那么他们也可以写一个计算引擎任务，还是从刚刚那个消息队列里拉数据，做分析。

计算引擎所面向的场景，往往本身并不新奇，反正永远是与 processing 相关的。正是由于当下数据规模之大，才使得我们不得不将一些传统手段改为分布式的手段，这样才能解决如此数据量的问题。

这个入门课程只带你看到了大数据冰山的一角。去妥善处理与运用这些数据，还涉及到存储、网络、编排调度等太多太多太多内容；这个领域大多都跑在 JVM 上，所以新的 C++ 引擎也逐渐向传统引擎发起了挑战。如果大家在未来有意愿做基础软件研发工作，那么大数据基础架构是一个可供你选择的方向。

