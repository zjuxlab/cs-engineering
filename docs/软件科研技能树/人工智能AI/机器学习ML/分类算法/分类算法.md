---
title: 分类算法
slug: 分类算法
sidebar_position: 3
---


# 分类算法

> 既然是「传统人工智能」，就不讲使用神经网络的算法了

# 前言

## 神经网络为王？

不是的；在SVM出现后的一段时间，一度力压神经网络成为了最为热点的算法；

事到如今，是算力和求解方法的突破似的神经网络重新登上王位；

当然，也不是什么机器学习都可以无脑上神经网络的，对于数据量较少，特征较明显的数据集，是可以使用一下更为直观的算法的。

## 分类和回归

二分类和回归，其实是等价的问题；

其本质都是在实数域上给出输入特征`X = {x1, ... , xn}` 与取值`y`

算法基于数据集`{(X,y)}`学习出一个函数`F`，使得对于同分布的X，可以准确估计`y = F(X)`

对于分类问题，y就是类的序号（二分类时通常取+1 -1）

而对于回归问题，y就是预测值

# KNN

k-Nearest Neighbors   k近邻算法

## 概述

![](/assets/CvyqbHn7hoGT2ox0DzFcb2avnAc.png)

严格意义上不算是学习算法，而是“死记硬背”算法

此算法没有学习过程，只是把数据原封不动（或者树状结构化地存下来）

而把复杂度全部放在了预测时

## 参数

- k  表示取几个最近邻进行统计

## 预测流程

对样本数据按照和待预测X的距离排序，选取最近的k个点

k个点中个数最多的y为预测的y

## 优点

实现简单（只需要一个查找排序的索引结构即可，可以使用kdTree等）

准确可靠（可证明：KNN是理论最优分类器的错误率二倍以内）

## 缺点

没有学习过程，预测时空间复杂度和时间复杂度高

维数高时难以找到足够贴近的样本点，准确度降低（维数灾难）

[机器学习(五):KNN与维度灾难_芙兰泣露的博客-CSDN博客_knn维度灾难](https://blog.csdn.net/u012882134/article/details/78203410)

# SVM

Support Vector Machine   支持向量机

## 概述

其本质是寻找一个N-1维超平面，将两类数据进行划分，使得<u>距离平面最近</u>的点到平面的<u>距离最大</u>

我们假设样本的`y \in {+1, -1}`(这垃圾飞书不能latex公式）

而计算所得的超平面为 `y = wx+b`

则待优化的目标如下：

通过优化平面参数，最大化边界距离`\gamma`，使得样本点到超平面的距离均大于`\gamma`

![](/assets/QRHxbgUKmodPw7x00EFcah3tnzg.png)

## 目标函数化简

由于平面的参数是可以缩放的，只要不为零，我们总可以通过倍乘w和b是的分子大于等于1

即简化为：

![](/assets/RfLgbAQecof2xUxIrGdc5WB4nwd.png)

取倒数，得到最小化问题：

![](/assets/Ulv9boOocouAnAxZniHciz56nsf.png)

对于此问题，使用「拉格朗日数乘法」，将条件并入待优化函数：

![](/assets/Ul5NbmAcKoOoMhx6gBNcupQ7nWe.png)

根据Minimax原理，minmax可以在<u>一定条件</u>（不太记得了欢迎补充）下交换位置：

![](/assets/HQddbg7txokfZMxXiYYcINl4nag.png)

![](/assets/BINSbP22loDKIdxhDlLcYr7RnYf.png)

![](/assets/B6pWbxgeIoc7kWxlGwIc4YPOnyc.png)

交换之后，对于内侧min，`\lambda`是常量，可以得到w b的极值条件

![](/assets/QiHKb13ppoXedLxoX9scBMUPnHd.png)

代入原式，化简外层max：

![](/assets/DMckbl0PXo8pLXxcdtRchsWfnng.png)

【KKT条件】

前两条是函数化简中得到的

第三条是根据min-max的博弈得到的，min想让每一项最小，使得它是负数；而max为了让这一项最大，则需要使这一项系数为0

![](/assets/MwrvbgLB7oKQ9Zxl1aAcXXqWn6g.png)

## SVM的求解

基于SMO求解，简单来说就是根据和为零的性质，每次变动其中两个lambda参数，求其极值；循环到收敛

![](/assets/G7cjbug3FoxjV8xdIUkckfTFnbg.png)

## 异常数据导致的线性不可分情况

以上问题之针对线性可分情况成立，否则最优间距为负数，无法得到合理结果。

因此，我们可以加入一个误差项，使得负距离在一定程度上被允许出现不允许结果

因此我们适当修改约束条件，使得一些变量的距离值重要性降低

![](/assets/Pjp9bdIOiobfY8xyBMZciGv5npY.png)

进一步简化，可以得到：

![](/assets/R2gfbq7p8oDTrGxJi3XchR2NnRc.png)

## 数据分布导致的线性不可分情况

当两类数据的分布边界本身就不是平面时，便需要将数据经过变换，再进行线性分类

这个过程类似于通过线性回归求解二次函数的回归

因此，我们引入核函数：

![](/assets/CVK0bTQwboJ7X9xAEsFciv6Unuc.png)

代入待优化的方程，取代x的点乘项，使得算法中出现非线性成分

![](/assets/ElY1bqvOeoxtovxhYgucB5Glnkh.png)

# Adaboost

AdaBoost本身并不是一个分类器，而是将多个低准确度的分类器组合成一个高准确度分类器的合并算法

【算法参数】

n：简单分类器个数，也是算法迭代次数

classifier[n]：n个简单分类器

OUT classifier_weight[n]：每个分类器最终权重

IN weight[X.size]：每个样本的权重值，默认为平均分布

## 算法流程

```py
for i in range(n):
    classifier[i].fit(X,y,weight)
    y2 = classifier[i].predict(X)
    update weight
    update classifier_weight[i]
```

### 分类器权重

假设`e_m`为第m个分类器的预测误差，则当前分类器的权重为

![](/assets/TBZKbHaYpoW3OVxFZA4ctP3bnUd.png)

### 样本权重

根据当前分类器的预测结果是否正确，对样本分别乘以exp(+-分类器权重)

![](/assets/SDNPbcMvVopDz2xhk9pcDGY5nWd.png)

## 预测期

根据分类器权重求和，得到最终输出，根据输出的离哪一类近决定属于哪一类

![](/assets/YX7jbWrBfoSOOdxdtEZcbCm3nmd.png)

