---
title: 回归
slug: 回归
sidebar_position: 2
---


# 回归

Author: 葛佳惠

## 一元线性回归

- 对于一组n个测量($X_i$,$Y_i$)值，认为Y与X符合线性关系

$$Y_i=b_0+b_1X_i+e_i$$

- $b_0$截距，$b_1$斜率
- Y的方差为: $var(Y) = var(e) =$$\sigma^2$
- 如何估计$b_0$和$b_1$?

## 最小二乘法（Least Squares）

希望Y的预测值与测量值偏差越小越好,对于n个测量

$$Q=\sum(Y_i−(b_0+b_1X_i))^2$$

最小化Q,最小二乘法

将回归函数的表达式改写为

$$Y=β_0+β_1(X−\bar{X})$$

上式被称为模型的中心化

$$Q=\sum[Y_i−(β_0+β_1(X_i−\bar{X}))]^2$$

对$β_0$和$β_1$求偏导

$$\frac{\partial Q}{\partial β_0}=−2\sum[Y_i−β_0−β_1(X_i−\bar{X})]=0$$

$$\frac{∂Q}{∂β_1}=−2∑[Y_i−β_0−β_1(Xi−\bar{X})](X_i−\bar{X})=0$$

得到

$$\hat{β_0}=∑Y_i/n=\bar{Y}$$

$$\hat{β_1}=\frac{∑(X_i−\bar{X})Y_i}{{S_X}^2}$$

$\hatβ_0$,$\hatβ_1$均为$Y_i$的线性函数

- $\hatβ_0$,$\hatβ_1$是$β_0$,$β_1$的无偏估计

$$var(\hatβ_0)=var(\bar{Y})=σ^2/n$$

$$var(\hatβ_1)=var(\frac{∑(X_i-\hat{X})Y_i}{{S_X}^2})\\=∑(\frac{(X_i−\bar{X})}{{S_X}^2})^2var(Y_i)\\=var(Y_i)/{S_X}^2=σ^2/{S_X}^2$$

- 截距$β_0$的方差由测量数据的样本决定
- $β_1$的方差由${S_x}^2$决定，X的点取得越紧密，方差越大，应尽量取分散的点
- 可以证明，$β_0$和$β_1$的协方差$cov(β_0,β_1)=0$

$$Y=β_0+β_1(X−\bar{X})$$

$$b_0=β_0−β_1\bar{X}$$

$$b_1=β_1$$

$$var(b_0)=var(β_0)+\bar{X}^2var(β_1)$$

$$var(b_1)=var(β_1)$$

## 残差与σ2的估计

参数估计的Y与测量值Y的差，

$$δ_i=Y_i−\hat Y_i$$

称为残差。是最小二乘法中的分量

- 残差用于检验回归函数与数据的符合程度，不随x有特定趋势
- 残差用于估计$σ^2$

$$Y=b_0+b_1X+e$$

$$var(Y)=var(e)=σ^2$$

- 如何估计$σ^2$?

$$σ^2=∑{δ_i}^2/(n−2)$$

- 当$e_i$符合正态分布$N(0,σ^2)$时，${\hat σ}^2/σ^2∼χ_{n-2}^2$
    - 可以用于估计拟合与实验数据的符合程度

```py
import scipy.stats as st
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

a=3
b=4
x=st.uniform(0,10).rvs(20) #生成[0,10]的20个随机样本
y=[np.random.normal(a*i+b,5) for i in x]

sns.regplot(x=x, y=y, ci=95) #画出回归线和95%的置信区间
```

![](/assets/QcEUbRBbso6n4kxW0ylcdomUnWc.png)

```
from scipy.stats import linregress
#手动计算斜率和截距
#sx=sum (x-xi)^2
sx=np.var(x)*len(x)
sy=np.var(y)*len(y)
meanx=np.mean(x)
meany=np.mean(y)
#b1=beta1
b1= sum((x-meanx)*y)/sx
#b0=beta0-b1*xbar=ybar-b1*xbar
b0= meany-b1*meanx
residual=sum((y-b0-b1*x)**2)
varb1 = residual/sx/(len(x)-2)
varb0 = residual/len(x)/(len(x)-2)+meanx**2*varb1
print("Calculation: slope=",b1,"err=",np.sqrt(varb1)) #斜率和斜率的标准误差
print("Calculation: intercept=",b0,"err=",np.sqrt(varb0)) #截距和截距的标准误差
result=linregress(x,y) #使用scipy的linregress来验证结果
print("Fit: slope=", result.slope,result.stderr)
print("Fit: intercept=",result.intercept,result.intercept_stderr)
```

```md
Calculation: slope= 2.673200837026146 err= 0.31747618192943394
Calculation: intercept= 5.216099892669153 err= 1.853101865852724
Fit: slope= 2.6732008370261453 0.31747618192943405
Fit: intercept= 5.216099892669158 1.853101865852725
```

## 多元线性回归

- 若Y由多个因素控制

$$Y=b_0+b_1X_1+b_2X_2⋯b_pX_p+e$$

- $b_k$称为Y对$x_k$的回归系数
- 测量了一组n个数据

$$(y_i,x_{1i},x_{2i},⋯,x_{pi}),i=1,2⋯n$$

- 写为$β_0$,⋯,$β_1$的中心化形式

$$Y=β_0+β_1X_1^*+β_2X_2^∗⋯b_pX_p^∗+e$$
- 其中$x_i^∗=x_i−\bar x$

$$\sum_{j=1}^{n}x_{ij}^*=0$$

$$b_i=β_i,(i=1⋯p)$$

$$b_0=β_0+\sum_{i=1}^{p}β_ix_i$$

- 为表示方便，用$x$指代$x^*$

## 多元线性函数的矩阵表示

- 用列向量来表示$β$和$Y$的测量值

$$β=\begin{pmatrix}β_0\\⋮\\β_p\end{pmatrix},Y=\begin{pmatrix}y_1\\⋮\\y_n\end{pmatrix}$$

- n为数据的个数，p为x（系数）的个数
- 用矩阵来表示X的测量值

$$X=\begin{pmatrix}1\ x_{11}⋯x_{1p}\\⋮⋯⋮\\1\ x_{n1}⋯x_{np}\end{pmatrix}$$

$$δ=\begin{pmatrix}y_1−β_0−x_{11}β_1−⋯−x_{1p}β_p\\⋮\\y_n−β_0−x_{n1}β_1−⋯−x_{np}β_p\end{pmatrix}$$

$$δ^2=(Y−Xβ)^⊺(Y−Xβ)$$

## 最小二乘法求解

$$δ^2=(Y−Xβ)^⊺(Y−Xβ)$$

$$\frac{∂δ^2}{∂β}=0$$

- 矩阵求导规则, x为列向量：

$$\frac{∂x^Ta}{∂x}=\frac{∂a^Tx}{∂x}=a$$

$$\frac{∂x^TAx}{∂x}=Ax+A^Tx$$

- 带入公式

$$\frac{∂(Y−Xβ)^⊺(Y−Xβ)}{∂β}\\=\frac{∂(Y^T(Y−Xβ)−β^TX^T(Y−Xβ))}{∂β}\\=X^TY−X^TY+(X^TX+X^TX)β=0$$

- 得到

$$X^TXβ=X^TY$$

$$β=(X^TX)^{−1}X^TY$$

令$L=(X^TX)$, $σ^2L^{−1}$为β的协方差矩阵，且有

- $$Var(β_0)=σ^2/n$$
- $$Var(β_i)=σ^2L_{ii}^{−1}$$
- $$Var(β_0,β_j)=0$$
- $$Var(β_i,β_j)=σ^2L_{ij}^{−1}$$
- $\sigma^2$的估计仍然可用残差来估计

$$δ^2=(Y−Xβ)^⊺(Y−Xβ)$$

$$σ^2=δ^2/(n−p−1)$$

$$δ^2/σ^2∼χ_{n−p−1}^2$$

## βj的区间估计

$$(\hat β_j−β_j)/(\hat σ\sqrt{L_{jj}^{−1}})∼t_{n−p−1}$$

```
%matplotlib notebook

import scipy.stats as st
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import linregress
from matplotlib.animation import FuncAnimation

np.random.seed(304)
a=4
b=4
c=10

x=st.uniform(0,10).rvs(20)
y=st.uniform(0,8).rvs(20)
z=[np.random.normal(a*i+b*j+c,5) for i,j in zip(x,y)]
```

将数据点用三维动图展示出来

```
def animate(dfn):
    ax.view_init(elev=32, azim=dfn)
    
fig = plt.figure(figsize=(4, 4))

ax = fig.add_subplot(111, projection='3d')
ax.plot(x, y, z, color='k', zorder=15, linestyle='none', marker='o', alpha=0.5)

anim = FuncAnimation(fig, animate, frames=range(0,360,2))
anim.save('data.gif',fps=3,dpi=200)

fig.show()
```

调用线性回归模型

```
from sklearn import linear_model
    
X=np.vstack((x, y)).T
ols = linear_model.LinearRegression()
model = ols.fit(X,z)

x_pred = np.linspace(0, 10, 50)  
y_pred = np.linspace(0, 8, 50)  
xx_pred, yy_pred = np.meshgrid(x_pred, y_pred) #转换为二维网格坐标
#flatten() change the 50x50 to 1x2500
model_viz = np.array([xx_pred.flatten(), yy_pred.flatten()]).T
print(model_viz.shape)
predicted = model.predict(model_viz) #进行预测

fig = plt.figure(figsize=(4, 4))

ax = fig.add_subplot(111, projection='3d')
ax.scatter(x, y, z,color='k',alpha=0.5 )
ax.plot(xx_pred.flatten(), yy_pred.flatten(), predicted, alpha=0.9) #画出预测的线性回归平面

anim = FuncAnimation(fig, animate, frames=range(0,360,3))
fig.show()
anim.save('fit.gif',fps=3,dpi=200)

print(model.intercept_,model.coef_)
```

## 矩阵运算求线性回归系数

按照公式手动求解

```
one=np.ones(20)
Xm=np.vstack([one,x,y]).T
Y = np.array(z).reshape(20,1)
b=np.linalg.solve(Xm.T@Xm,Xm.T@Y)
print(b)
```

## 矩阵运算求线性回归误差

手动求解

```py
Xmm=np.vstack([one,x-np.mean(x),y-np.mean(y)]).T
beta=np.linalg.solve(Xmm.T@Xmm,Xmm.T@Y)
print(beta,"\n")
L_inv=np.linalg.inv(Xmm.T@Xmm)
sigma2=(Y-Xmm@beta).T @(Y-Xmm@beta)/(20-2-1)
print(L_inv,"\n")
err=np.sqrt(sigma2*np.diag(L_inv))
print(err)
```

调用模型

```py
import statsmodels.api as sm
    
#Xp=np.vstack((np.ones(20),x, y)).T
Xp = sm.add_constant(X) #添加一个常数列

# Ordinary least square
model = sm.OLS(z, Xp)
results = model.fit()

print(results.summary())
```

```
OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.825
Model:                            OLS   Adj. R-squared:                  0.804
Method:                 Least Squares   F-statistic:                     40.08
Date:                Mon, 21 Nov 2022   Prob (F-statistic):           3.68e-07
Time:                        12:23:46   Log-Likelihood:                -65.763
No. Observations:                  20   AIC:                             137.5
Df Residuals:                      17   BIC:                             140.5
Df Model:                           2                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          7.5468      4.245      1.778      0.093      -1.410      16.504
x1             3.8899      0.583      6.671      0.000       2.660       5.120
x2             4.2203      0.734      5.746      0.000       2.671       5.770
==============================================================================
Omnibus:                        0.209   Durbin-Watson:                   2.452
Prob(Omnibus):                  0.901   Jarque-Bera (JB):                0.292
Skew:                           0.204   Prob(JB):                        0.864
Kurtosis:                       2.571   Cond. No.                         18.6
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
```

